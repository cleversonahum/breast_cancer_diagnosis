{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/ufpa_logo.png\" alt=\"drawing\" width=\"120\"/></center>\n",
    "\n",
    "<h4 style=\"text-align: center;font-size:22px\"><strong>Universidade Federal do Par&aacute;</strong></h4>\n",
    "<h4 style=\"text-align: center;font-size:22px\"><strong>Programa de P&oacute;s-gradua&ccedil;&atilde;o em Engenharia El&eacute;trica</strong></h4>\n",
    "<h3 style=\"text-align: center;\">Evaluation of computationally intelligent techniques for breast cancer diagnosis</h3>\n",
    "<h4 style=\"text-align: center;font-size:24px\"><strong>Published on Neural Computing and Applications journal (2021)</strong></h4>\n",
    "<p>&nbsp;</p>\n",
    "<blockquote>\n",
    "<ul>\n",
    "<li><strong>Adilson</strong></li>\n",
    "<li><strong>Cleverson</strong></li>\n",
    "<li><strong>Felipe</strong></li>\n",
    "<li><strong>Rodrigo</strong></li>\n",
    "</ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "data_csv = \"../breast-cancer-wisconsin.csv\"\n",
    "df = pd.read_csv(data_csv, names=['ID','Clump','U_Cell_size','U_Cell_shape','Marginal_Adhesion','SE_epitelial_cell_size','Bare_nuclei','bland_chromatin','Normal_Nucleoli','Mitoses','Class'])\n",
    "#print('Dataset shape: ', df.shape)\n",
    "#print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Attribute                     Domain\n",
    "-- -----------------------------------------\n",
    "1. Sample code number            id number\n",
    "2. Clump Thickness               1 - 10\n",
    "3. Uniformity of Cell Size       1 - 10\n",
    "4. Uniformity of Cell Shape      1 - 10\n",
    "5. Marginal Adhesion             1 - 10\n",
    "6. Single Epithelial Cell Size   1 - 10\n",
    "7. Bare Nuclei                   1 - 10\n",
    "8. Bland Chromatin               1 - 10\n",
    "9. Normal Nucleoli               1 - 10\n",
    "10. Mitoses                       1 - 10\n",
    "11. Class:                        (0 for benign, 1 for malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "null_columns=df.columns[df.isnull().any()]\n",
    "print(df[df.isnull().any(axis=1)][null_columns].head())\n",
    "# Change bare nuclei values to numeric only and check the numbers of NAN\n",
    "df['Bare_nuclei'] = pd.to_numeric(df['Bare_nuclei'], errors='coerce', downcast='integer')\n",
    "print(df['Bare_nuclei'].isnull().values.sum())\n",
    "\n",
    "# Drop the lines with null values\n",
    "df = df.dropna()\n",
    "print(df['Bare_nuclei'].isnull().values.sum())\n",
    "\n",
    "# Removing ID column since it won't be considered to the training\n",
    "df.pop('ID')\n",
    "\n",
    "# So, the dataset removing null values and ID column has the size:\n",
    "print('New dataset shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Values distribution\n",
    "for name, values in df.iteritems():\n",
    "    print (name, '\\nMin Value:  ', np.min(values), '\\nMax Value: ', np.max(values), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Changing Class label from 2 or 4 to 0 or 1\n",
    "df.loc[:, 'Class'] = np.where(df['Class']==2, 0, 1)\n",
    "print ('Class\\nMin Value:  ', np.min(df['Class']), '\\nMax Value: ', np.max(df['Class']), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total     = {len(df)} -> 100%\")\n",
    "print(f\"Benign    = {len(df[df.Class == 0])} -> {len(df[df.Class == 0])/len(df) *100}%\")\n",
    "print(f\"Malignant = {len(df[df.Class == 1])} -> {len(df[df.Class == 1])/len(df) *100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Atributes correlation for benign and malignant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation benign samples\n",
    "corr_mat_benign = df.loc[df.Class == 0].drop(['Class'], axis=1).corr()\n",
    "corr_mat_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.heatmap(corr_mat_benign, linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corr_mat_malignant = df.loc[df.Class == 1].drop(['Class'], axis=1).corr()\n",
    "corr_mat_malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.heatmap(corr_mat_malignant, linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate entries from outputs\n",
    "dataset = df.to_numpy(dtype=np.int) # Converting from Pandas dataframe to Numpy\n",
    "entries = dataset[:, 0:9]\n",
    "outputs = dataset[:, 9]\n",
    "print(entries.shape)\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset between train and test\n",
    "seed = 10 # Set seed to get invariant results\n",
    "test_size = 0.34\n",
    "x_train, x_test, y_train, y_test = train_test_split(entries, outputs, test_size=test_size, random_state=seed)\n",
    "print('Train dataset shape:\\nEntries: ', x_train.shape, '\\nOutput: ', y_train.shape, '\\n\\n')\n",
    "print('Test dataset shape:\\nEntries: ', x_test.shape, '\\nOutput: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create 10-fold validation set for training\n",
    "K = 10\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Tree classifier\n",
    "dt = tree.DecisionTreeClassifier(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=20, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Organizing classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = [(dt, \"Decision tree\"), (rf, \"Random forest\"), (neigh, \"K-Nearest neighbor\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classifiers training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training classifiers using cross-validation\n",
    "fold_number = 1\n",
    "for train_indexes, valid_indexes in kf.split(x_train):\n",
    "    print(\"Fold \", fold_number)\n",
    "    for classifier, label in classifiers:\n",
    "        classifier.fit(x_train[train_indexes], y_train[train_indexes])\n",
    "        y_valid_pred = classifier.predict(x_train[valid_indexes])\n",
    "        print(\"Classifier type: \",label, \", Validation Accuracy = \", accuracy_score(y_train[valid_indexes], y_valid_pred))\n",
    "    print('\\n')\n",
    "    fold_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Testing classifiers\n",
    "for classifier, label in classifiers:\n",
    "    y_test_estimative = classifier.predict(x_test)\n",
    "    print(\"Classifier type: \", label, \", Test Accuracy = \", accuracy_score(y_test, y_test_estimative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Confusion matrix calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrixes = np.zeros((len(classifiers), 4))\n",
    "for index, classifier_info in enumerate(classifiers):\n",
    "    confusion_matrixes[index,:] = np.array([confusion_matrix(outputs, classifier_info[0].predict(entries)).ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(dataframe, metric_indexes, indexes_results, orientation, xlabel, ylabel):\n",
    "    classifier_labels = [\"Decision tree\", \"Random forest\", \"K-Nearest neighbor\"]\n",
    "    df_perf_results = pd.DataFrame(dataframe, columns=metric_indexes)\n",
    "    df_perf_results.insert(0, 'classifier_type', classifier_labels, True)\n",
    "    df_perf_results = pd.melt(df_perf_results, id_vars=['classifier_type'], value_vars=indexes_results, var_name='Metric')\n",
    "    \n",
    "    # Plot confusion matrixes for each classifier\n",
    "    plt.figure()\n",
    "    if orientation==\"h\":\n",
    "        x = \"value\"\n",
    "        y = \"classifier_type\"\n",
    "        gridon = \"x\"\n",
    "    else:\n",
    "        y = \"value\"\n",
    "        x = \"classifier_type\"\n",
    "        gridon = \"y\"\n",
    "    x = \"value\" if orientation==\"h\" else \"classifier_type\"\n",
    "    y = \"classifier_type\" if orientation==\"h\" else \"value\"\n",
    "    sns.catplot(data=df_perf_results, kind=\"bar\", orient=orientation, x=x, y=y, hue=\"Metric\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "    plt.ylabel(xlabel, fontsize=14)\n",
    "    plt.xlabel(ylabel, fontsize=14)\n",
    "    plt.grid(axis=gridon)\n",
    "    plt.show()\n",
    "\n",
    "# Generate dataset to plot using seaborn package\n",
    "indexes = [\"TP\", \"FN\", \"FP\", \"TN\"]\n",
    "indexes_result1 = [\"TP\", \"FP\", \"FN\", \"TN\"]\n",
    "\n",
    "plot_metrics(confusion_matrixes, indexes, indexes_result1, \"v\", \"Classifiers\", \"No. of samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_confusion_matrixes = pd.DataFrame(confusion_matrixes, columns=indexes, index=[label for _, label in classifiers])\n",
    "df_confusion_matrixes.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculates the performance metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def perf_metrics(confusion_values):\n",
    "    # [0] = TP, [1] = FN , [2] = FP, [3] = TN\n",
    "    # 4.1 accuracy\n",
    "    accuracy = (confusion_values[0] + confusion_values[3]) / (np.sum(confusion_values))\n",
    "    # 4.2 precision\n",
    "    precision = confusion_values[0] / (confusion_values[0] + confusion_values[2])\n",
    "    # 4.3 specificity\n",
    "    specificity = confusion_values[3] / (confusion_values[3] + confusion_values[2])\n",
    "    # 4.4 TP rate\n",
    "    tp_rate = confusion_values[0] / (confusion_values[0] + confusion_values[1])\n",
    "    # 4.5 FP rate\n",
    "    fp_rate = confusion_values[2] / (confusion_values[2] + confusion_values[3])\n",
    "    # 4.6 NPV\n",
    "    npv = confusion_values[3] / (confusion_values[3] + confusion_values[1])\n",
    "    # 4.7 Rate of Misclassification\n",
    "    misclassification_rate = (confusion_values[2] + confusion_values[1]) / (np.sum(confusion_values))\n",
    "    # 4.8 F1 Score\n",
    "    f1_score = (precision * tp_rate) / (precision + tp_rate)\n",
    "\n",
    "    return np.array([accuracy, precision, specificity, tp_rate, fp_rate, npv, misclassification_rate, f1_score])\n",
    "\n",
    "perf_results = np.zeros((confusion_matrixes.shape[0], 8))\n",
    "for i in np.arange(confusion_matrixes.shape[0]):\n",
    "    perf_results[i,:] = perf_metrics(confusion_matrixes[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "metric_indexes = [\"CA\", \"Pre\", \"Spec\", \"Rec\", \"FPR\", \"NPV\", \"RMC\", \"F1\"] # that stands for Classification accuracy, Precision, Specificity, Recall/TP rate, \n",
    "# False positive rate, negative predictive value, misclassification rate and F1, respectively.\n",
    "indexes_result2 = [\"RMC\", \"FPR\", \"NPV\", \"F1\", \"Spec\"]\n",
    "\n",
    "plot_metrics(perf_results, metric_indexes, indexes_result2, \"h\", \"No. of samples\", \"Classifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_perf_results = pd.DataFrame(perf_results, columns=metric_indexes, index=[label for _, label in classifiers])\n",
    "df_perf_results[indexes_result2].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indexes_result3 = [\"CA\", \"Pre\", \"Rec\"]\n",
    "plot_metrics(perf_results, metric_indexes, indexes_result3, \"h\", \"No. of samples\", \"Classifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_perf_results = pd.DataFrame(perf_results, columns=metric_indexes, index=[label for _, label in classifiers])\n",
    "df_perf_results[indexes_result3].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Code to create ROC curve\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for index, classifier_info in enumerate(classifiers):\n",
    "    \n",
    "    fpr[classifier_info[1]], tpr[classifier_info[1]], _ = roc_curve(outputs, classifier_info[0].predict(entries))\n",
    "    roc_auc[classifier_info[1]] = auc(fpr[classifier_info[1]], tpr[classifier_info[1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot curves\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[\"Decision tree\"], tpr[\"Decision tree\"], color='blue',\n",
    "         lw=lw, label='Decision tree ROC curve (area = %0.2f)' % roc_auc[\"Decision tree\"])\n",
    "\n",
    "plt.plot(fpr[\"Random forest\"], tpr[\"Random forest\"], color='deeppink',\n",
    "         lw=lw, label='Random forest ROC curve (area = %0.2f)' % roc_auc[\"Random forest\"])\n",
    "\n",
    "plt.plot(fpr[\"K-Nearest neighbor\"], tpr[\"K-Nearest neighbor\"], color='darkcyan',\n",
    "         lw=lw, label='K-Nearest neighbor ROC curve (area = %0.2f)' % roc_auc[\"K-Nearest neighbor\"])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plot ROC Curves in subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot curves in subplots\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "############### 1\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(fpr[\"Decision tree\"], tpr[\"Decision tree\"], color='blue',\n",
    "         lw=lw, label='Decision tree ROC')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "############## 2\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(fpr[\"Random forest\"], tpr[\"Random forest\"], color='deeppink',\n",
    "         lw=lw, label='Random forest ROC')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "############ 3\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(fpr[\"K-Nearest neighbor\"], tpr[\"K-Nearest neighbor\"], color='darkcyan',\n",
    "         lw=lw, label='K-Nearest neighbor ROC')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Paper negative points  (Let's make a list to use after)\n",
    "- Does not mention the null values present at `Bare_nuclei` atribute\n",
    "- Does not treat the proportion of benign and malign samples at training and test dataset\n",
    "- We assume that `ID` column is not used to the training, but paper seems to consider it\n",
    "- Normalization of entry values could improve the performance for some methods as NN\n",
    "- Use a repeated K-fold cross validation could improve the performance\n",
    "- Use grid-search to tune decision tree parameters could be implemented\n",
    "- Does not specify any of the Decision tree parameters used (or they just use the default implementation without set any parameters)\n",
    "- Calculates the decision matrix over the entire dataset\n",
    "- Authors didn't remove NaN values before calculate the correlation\n",
    "- Authors used the wrong labels at confusion matrixes of fig. 15, the right sequence is TN, FP, FN and TP.\n",
    "- Authors plotted 2 times the F1 at both figure 16 and 17, and forgot to plot precision, and also plot AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks slide"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "livereveal": {
   "enable_chalkboard": true,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "theme": "moon"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
